# -*- coding: utf-8 -*-
"""v2_hack_SanityCheckAI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PSzhavt87PFVZTDCe4ahW2B4uRK587Yk
"""

# Mount Google Drive (optional, recommended for saving results)
from google.colab import drive
drive.mount('/content/drive')

# Download the mixed species dataset (E. coli + S. aureus + B. subtilis)
# This is 149 MB - perfect size for Colab!
!wget https://zenodo.org/records/5551009/files/DeepBacs_Data_Segmentation_StarDist_MIXED_dataset.zip

# Unzip the dataset
!unzip DeepBacs_Data_Segmentation_StarDist_MIXED_dataset.zip -d bacterial_data/

# Explore the dataset structure
!ls -R bacterial_data/

# Check image count
!find bacterial_data/ -name "*.tif" | wc -l

#!/usr/bin/env python3
"""
Explore the DeepBacs Mixed Species Dataset
Dataset contains: S. aureus (cocci), E. coli (rods), B. subtilis (rods)
Perfect for bacterial culture QC contamination detection!
"""

import os
import glob
from pathlib import Path
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches

# Dataset structure
DATA_DIR = "bacterial_data"

def analyze_dataset_structure():
    """Analyze the dataset folder structure"""
    print("=" * 70)
    print("DATASET STRUCTURE ANALYSIS")
    print("=" * 70)

    # Count files
    train_source = glob.glob(f"{DATA_DIR}/training/source/*.tif")
    train_target = glob.glob(f"{DATA_DIR}/training/target/*.tif")
    test_source = glob.glob(f"{DATA_DIR}/test/source/*.tif")
    test_target = glob.glob(f"{DATA_DIR}/test/target/*.tif")

    print(f"\nüìÅ Training Set:")
    print(f"   Source images: {len(train_source)}")
    print(f"   Target masks:  {len(train_target)}")

    print(f"\nüìÅ Test Set:")
    print(f"   Source images: {len(test_source)}")
    print(f"   Target masks:  {len(test_target)}")

    print(f"\nüìä Total images: {len(train_source) + len(test_source)}")
    print(f"üìä Total masks:  {len(train_target) + len(test_target)}")

    return train_source, train_target, test_source, test_target


def identify_species(filename):
    """Identify bacterial species from filename"""
    filename = filename.lower()

    if 'je2' in filename or 'nile' in filename or '_nr' in filename:
        return 'S. aureus (cocci)', 'red'
    elif 'pos' in filename or 'train_' in filename:
        # E. coli or B. subtilis - need to look at morphology
        return 'E. coli / B. subtilis (rods)', 'blue'
    elif 'test_' in filename:
        return 'Mixed/Unknown', 'green'
    else:
        return 'Unknown', 'gray'


def analyze_image_properties(image_paths, subset_name):
    """Analyze image properties"""
    print(f"\n{'='*70}")
    print(f"{subset_name} - IMAGE PROPERTIES")
    print(f"{'='*70}")

    # Sample 10 images
    sample_paths = np.random.choice(image_paths, min(10, len(image_paths)), replace=False)

    sizes = []
    dtypes = []
    value_ranges = []

    for img_path in sample_paths:
        img = Image.open(img_path)
        img_array = np.array(img)

        sizes.append(img.size)
        dtypes.append(img_array.dtype)
        value_ranges.append((img_array.min(), img_array.max()))

    print(f"\nüìê Image Sizes: {set(sizes)}")
    print(f"üî¢ Data Types: {set(dtypes)}")
    print(f"üìä Value Ranges (min, max): {value_ranges[:3]}...")

    return sizes, dtypes, value_ranges


def count_species_distribution(image_paths):
    """Count distribution of different bacterial species"""
    print(f"\n{'='*70}")
    print(f"SPECIES DISTRIBUTION")
    print(f"{'='*70}")

    species_counts = {}

    for img_path in image_paths:
        species, _ = identify_species(os.path.basename(img_path))
        species_counts[species] = species_counts.get(species, 0) + 1

    print("\nü¶† Species Distribution:")
    for species, count in sorted(species_counts.items()):
        print(f"   {species}: {count} images")

    return species_counts


def visualize_samples(train_source, train_target, test_source, test_target, num_samples=6):
    """Visualize sample images with their masks"""
    print(f"\n{'='*70}")
    print(f"VISUALIZING SAMPLE IMAGES")
    print(f"{'='*70}")

    # Select diverse samples
    samples = []

    # Get S. aureus samples
    aureus_samples = [p for p in train_source if 'je2' in p.lower()]
    if aureus_samples:
        samples.append(('S. aureus (DIC)', aureus_samples[0]))

    # Get E. coli / B. subtilis samples
    ecoli_samples = [p for p in train_source if 'pos' in p.lower() or 'train_' in p.lower()]
    if len(ecoli_samples) >= 2:
        samples.append(('E. coli/B. subtilis (Brightfield)', ecoli_samples[0]))
        samples.append(('E. coli/B. subtilis (Brightfield)', ecoli_samples[10]))

    # Get test samples
    if len(test_source) >= 2:
        samples.append(('Test Sample', test_source[0]))
        samples.append(('Test Sample', test_source[5]))

    # Create figure
    fig, axes = plt.subplots(len(samples), 2, figsize=(12, 3*len(samples)))
    if len(samples) == 1:
        axes = axes.reshape(1, -1)

    for idx, (label, source_path) in enumerate(samples):
        # Load source image
        source_img = Image.open(source_path)
        source_array = np.array(source_img)

        # Find corresponding target mask
        basename = os.path.basename(source_path)
        target_path = source_path.replace('/source/', '/target/')

        # Display source
        axes[idx, 0].imshow(source_array, cmap='gray')
        axes[idx, 0].set_title(f'{label}\nSource: {basename[:40]}...', fontsize=10)
        axes[idx, 0].axis('off')

        # Display target mask if exists
        if os.path.exists(target_path):
            target_img = Image.open(target_path)
            target_array = np.array(target_img)

            # Overlay mask on image
            axes[idx, 1].imshow(source_array, cmap='gray', alpha=0.7)
            axes[idx, 1].imshow(target_array, cmap='jet', alpha=0.3)
            axes[idx, 1].set_title(f'Segmentation Mask\nUnique cells: {len(np.unique(target_array)) - 1}',
                                   fontsize=10)
            axes[idx, 1].axis('off')
        else:
            axes[idx, 1].text(0.5, 0.5, 'No mask available',
                             ha='center', va='center', transform=axes[idx, 1].transAxes)
            axes[idx, 1].axis('off')

    plt.tight_layout()
    plt.savefig('/mnt/user-data/outputs/dataset_samples.png', dpi=150, bbox_inches='tight')
    print("\n‚úÖ Sample visualization saved to: dataset_samples.png")

    return fig


def create_species_comparison(train_source):
    """Create side-by-side comparison of different bacterial species"""
    print(f"\n{'='*70}")
    print(f"CREATING SPECIES COMPARISON")
    print(f"{'='*70}")

    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    # S. aureus (cocci - round)
    aureus_samples = [p for p in train_source if 'je2' in p.lower() and '_nr' not in p.lower()]
    if aureus_samples:
        img = Image.open(aureus_samples[0])
        axes[0].imshow(np.array(img), cmap='gray')
        axes[0].set_title('S. aureus (Cocci)\nRound/spherical cells', fontsize=12, fontweight='bold')
        axes[0].axis('off')

    # E. coli (rods)
    ecoli_samples = [p for p in train_source if 'pos' in p.lower()]
    if ecoli_samples:
        img = Image.open(ecoli_samples[0])
        axes[1].imshow(np.array(img), cmap='gray')
        axes[1].set_title('E. coli (Rods)\nElongated rod-shaped cells', fontsize=12, fontweight='bold')
        axes[1].axis('off')

    # B. subtilis (rods, often longer)
    bsub_samples = [p for p in train_source if 'train_' in p.lower()]
    if bsub_samples:
        img = Image.open(bsub_samples[5])
        axes[2].imshow(np.array(img), cmap='gray')
        axes[2].set_title('B. subtilis (Rods)\nLonger rod-shaped cells', fontsize=12, fontweight='bold')
        axes[2].axis('off')

    plt.suptitle('Bacterial Morphology Comparison\n(Perfect for Contamination Detection!)',
                 fontsize=14, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.savefig('/mnt/user-data/outputs/species_comparison.png', dpi=150, bbox_inches='tight')
    print("‚úÖ Species comparison saved to: species_comparison.png")

    return fig


def generate_qc_strategy():
    """Generate a QC detection strategy based on dataset"""
    print(f"\n{'='*70}")
    print(f"üéØ YOUR QC DETECTION STRATEGY")
    print(f"{'='*70}")

    strategy = """
    üìã BACTERIAL CULTURE QC CLASSES

    ‚úÖ CLASS 1: NORMAL (Pure Culture)
    - Use: Pure E. coli images (rod-shaped, uniform morphology)
    - Files: pos*.tif, train_*.tif (brightfield E. coli)
    - Characteristics: Uniform rod shapes, consistent size

    ‚ö†Ô∏è CLASS 2: CONTAMINATION (Mixed Species)
    - Use: Mix S. aureus (cocci) + E. coli (rods) images
    - Files: Combine JE2* (cocci) with pos*/train_* (rods)
    - Characteristics: Mixed morphologies (round + rod shapes)

    ‚ùå CLASS 3: POOR GROWTH
    - Use: Low-density images or create from sparse regions
    - Strategy: Filter images with low cell counts
    - Characteristics: Few cells, poor coverage

    üîß CLASS 4: AUTOMATION ERROR
    - Generate synthetically:
      * Blank wells (empty images)
      * Partial coverage (crop portions)
      * Out-of-focus (blur filters)

    üéØ KEY ADVANTAGE:
    This dataset is LABEL-FREE (brightfield/DIC microscopy)
    ‚úÖ Compatible with non-destructive automated culture monitoring
    ‚úÖ Perfect for production workflows (Opentrons, Monomer)
    ‚úÖ No staining required = cells stay alive!
    """

    print(strategy)

    return strategy


def main():
    """Main analysis function"""
    print("\n" + "üß´" * 35)
    print("  BACTERIAL CULTURE QC DATASET EXPLORER")
    print("  DeepBacs Mixed Species Dataset")
    print("üß´" * 35)

    # 1. Analyze structure
    train_source, train_target, test_source, test_target = analyze_dataset_structure()

    # 2. Analyze image properties
    analyze_image_properties(train_source, "TRAINING SET")
    analyze_image_properties(test_source, "TEST SET")

    # 3. Count species distribution
    count_species_distribution(train_source + test_source)

    # 4. Visualize samples
    visualize_samples(train_source, train_target, test_source, test_target)

    # 5. Create species comparison
    create_species_comparison(train_source)

    # 6. Generate QC strategy
    generate_qc_strategy()

    print(f"\n{'='*70}")
    print("‚úÖ ANALYSIS COMPLETE!")
    print(f"{'='*70}")
    print("\nüìä Generated Files:")
    print("   ‚Ä¢ dataset_samples.png - Sample images with masks")
    print("   ‚Ä¢ species_comparison.png - Morphology comparison")
    print("\nüéØ Next Steps:")
    print("   1. Review the images to understand morphology differences")
    print("   2. Start building your QC classifier")
    print("   3. Use cocci vs rods distinction for contamination detection!")
    print(f"{'='*70}\n")


if __name__ == "__main__":
    main()

import numpy as np
from PIL import Image
import glob
import os
from scipy import ndimage

# Create output directory
os.makedirs('contamination_data_realistic', exist_ok=True)

# Get image lists
s_aureus = [f for f in glob.glob('bacterial_data/training/source/JE2*.tif')
            if '_NR' not in f]
e_coli = glob.glob('bacterial_data/training/source/pos*.tif')

print(f"Found {len(s_aureus)} S. aureus images")
print(f"Found {len(e_coli)} E. coli images")

def create_realistic_contamination(cocci_path, rods_path, contamination_level='medium'):
    """
    Create realistic contamination by adding patches of cocci to rods image

    contamination_level: 'light', 'medium', 'heavy'
    """
    cocci = np.array(Image.open(cocci_path))
    rods = np.array(Image.open(rods_path))

    # Resize cocci to match rods
    if cocci.shape != rods.shape:
        cocci = np.array(Image.fromarray(cocci).resize((rods.shape[1], rods.shape[0])))

    # Start with rods as the base (this is the "intended" culture)
    output = np.copy(rods)

    h, w = cocci.shape

    # Determine contamination parameters based on level
    if contamination_level == 'light':
        num_patches = np.random.randint(1, 3)
        size_range = (40, 80)
    elif contamination_level == 'medium':
        num_patches = np.random.randint(2, 4)
        size_range = (60, 120)
    else:  # heavy
        num_patches = np.random.randint(3, 6)
        size_range = (80, 150)

    # Add circular/elliptical patches of cocci (contamination colonies)
    for _ in range(num_patches):
        # Random location
        center_y = np.random.randint(size_range[1], h - size_range[1])
        center_x = np.random.randint(size_range[1], w - size_range[1])

        # Random size (elliptical)
        radius_y = np.random.randint(size_range[0], size_range[1])
        radius_x = np.random.randint(size_range[0], size_range[1])

        # Create elliptical mask
        y, x = np.ogrid[:h, :w]
        mask = ((x - center_x)**2 / radius_x**2 +
                (y - center_y)**2 / radius_y**2) <= 1

        # Smooth the edges for more natural transition
        mask_smooth = ndimage.gaussian_filter(mask.astype(float), sigma=5)

        # Blend cocci into the image using smooth mask
        output = (mask_smooth[:, :, np.newaxis] * cocci[:, :, np.newaxis] +
                  (1 - mask_smooth[:, :, np.newaxis]) * output[:, :, np.newaxis]).squeeze()
        output = output.astype(np.uint8)

    return output

# Generate contamination images with different levels
num_light = 10
num_medium = 15
num_heavy = 10

print("\nGenerating realistic contamination images...")
print(f"  Light contamination:  {num_light} images")
print(f"  Medium contamination: {num_medium} images")
print(f"  Heavy contamination:  {num_heavy} images")

# Light contamination
for i in range(num_light):
    cocci_path = np.random.choice(s_aureus)
    rods_path = np.random.choice(e_coli)

    mixed = create_realistic_contamination(cocci_path, rods_path, 'light')

    output_path = f'contamination_data_realistic/light_{i:03d}.tif'
    Image.fromarray(mixed).save(output_path)

print(f"‚úì Generated {num_light} light contamination images")

# Medium contamination
for i in range(num_medium):
    cocci_path = np.random.choice(s_aureus)
    rods_path = np.random.choice(e_coli)

    mixed = create_realistic_contamination(cocci_path, rods_path, 'medium')

    output_path = f'contamination_data_realistic/medium_{i:03d}.tif'
    Image.fromarray(mixed).save(output_path)

print(f"‚úì Generated {num_medium} medium contamination images")

# Heavy contamination
for i in range(num_heavy):
    cocci_path = np.random.choice(s_aureus)
    rods_path = np.random.choice(e_coli)

    mixed = create_realistic_contamination(cocci_path, rods_path, 'heavy')

    output_path = f'contamination_data_realistic/heavy_{i:03d}.tif'
    Image.fromarray(mixed).save(output_path)

print(f"‚úì Generated {num_heavy} heavy contamination images")

print("\n" + "="*60)
print("DONE! Created realistic contamination dataset")
print("="*60)
print(f"Location: contamination_data_realistic/")
print(f"Total:    {num_light + num_medium + num_heavy} images")
print(f"  - Light:  {num_light} images (1-2 small colonies)")
print(f"  - Medium: {num_medium} images (2-4 medium colonies)")
print(f"  - Heavy:  {num_heavy} images (3-6 large colonies)")
print("="*60)

import matplotlib.pyplot as plt

# Visualize the different contamination levels
fig, axes = plt.subplots(4, 6, figsize=(18, 12))
fig.suptitle('Realistic Contamination Dataset (Patch-Based Method)',
             fontsize=16, fontweight='bold')

# Row 0: Pure E. coli (baseline/normal)
e_coli_samples = glob.glob('bacterial_data/training/source/pos*.tif')[:6]
for i, path in enumerate(e_coli_samples):
    img = Image.open(path)
    axes[0, i].imshow(img, cmap='gray')
    axes[0, i].set_title(f'Normal (Pure E. coli)', fontsize=9, color='green')
    axes[0, i].axis('off')

# Row 1: Light contamination
light_samples = glob.glob('contamination_data_realistic/light_*.tif')[:6]
for i, path in enumerate(light_samples):
    img = Image.open(path)
    axes[1, i].imshow(img, cmap='gray')
    axes[1, i].set_title(f'Light Contamination', fontsize=9, color='orange')
    axes[1, i].axis('off')

# Row 2: Medium contamination
medium_samples = glob.glob('contamination_data_realistic/medium_*.tif')[:6]
for i, path in enumerate(medium_samples):
    img = Image.open(path)
    axes[2, i].imshow(img, cmap='gray')
    axes[2, i].set_title(f'Medium Contamination', fontsize=9, color='darkorange')
    axes[2, i].axis('off')

# Row 3: Heavy contamination
heavy_samples = glob.glob('contamination_data_realistic/heavy_*.tif')[:6]
for i, path in enumerate(heavy_samples):
    img = Image.open(path)
    axes[3, i].imshow(img, cmap='gray')
    axes[3, i].set_title(f'Heavy Contamination', fontsize=9, color='red')
    axes[3, i].axis('off')

# Add row labels
axes[0, 0].text(-0.25, 0.5, 'NORMAL\n(Baseline)',
                transform=axes[0, 0].transAxes,
                fontsize=11, fontweight='bold', color='green',
                verticalalignment='center', rotation=90)

axes[1, 0].text(-0.25, 0.5, 'LIGHT\nContamination',
                transform=axes[1, 0].transAxes,
                fontsize=11, fontweight='bold', color='orange',
                verticalalignment='center', rotation=90)

axes[2, 0].text(-0.25, 0.5, 'MEDIUM\nContamination',
                transform=axes[2, 0].transAxes,
                fontsize=11, fontweight='bold', color='darkorange',
                verticalalignment='center', rotation=90)

axes[3, 0].text(-0.25, 0.5, 'HEAVY\nContamination',
                transform=axes[3, 0].transAxes,
                fontsize=11, fontweight='bold', color='red',
                verticalalignment='center', rotation=90)

plt.tight_layout()
plt.savefig('contamination_levels_visualization.png', dpi=150, bbox_inches='tight')
plt.show()

print("\nVisualization saved as 'contamination_levels_visualization.png'")

# STEP 1: Load the realistic v2 contamination data
print("="*70)
print("LOADING REALISTIC CONTAMINATION DATA (v2)")
print("="*70)

import pandas as pd
import numpy as np
from PIL import Image
import glob
from sklearn.model_selection import train_test_split

# Collect all file paths and labels
data_v2 = []

# Normal samples (label = 0)
normal_files = glob.glob('bacterial_data/training/source/pos*.tif')
for filepath in normal_files:
    data_v2.append({
        'filepath': filepath,
        'label': 0,
        'category': 'normal'
    })

# Realistic Contaminated samples (label = 1) - from v2
contam_files_v2 = (glob.glob('contamination_data_realistic/light_*.tif') +
                   glob.glob('contamination_data_realistic/medium_*.tif') +
                   glob.glob('contamination_data_realistic/heavy_*.tif'))

for filepath in contam_files_v2:
    data_v2.append({
        'filepath': filepath,
        'label': 1,
        'category': 'contaminated'
    })

# Create DataFrame
df_v2 = pd.DataFrame(data_v2)

print(f"\nDataset Summary:")
print(f"  Total images: {len(df_v2)}")
print(f"  Normal:       {len(df_v2[df_v2['label'] == 0])} images")
print(f"  Contaminated: {len(df_v2[df_v2['label'] == 1])} images")

# Split data
train_df_v2, temp_df = train_test_split(df_v2, test_size=0.3, random_state=42, stratify=df_v2['label'])
val_df_v2, test_df_v2 = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])

print(f"\nData Split:")
print(f"  Training:   {len(train_df_v2)} images")
print(f"  Validation: {len(val_df_v2)} images")
print(f"  Test:       {len(test_df_v2)} images")

# STEP 2: Load images
print("\n" + "="*70)
print("LOADING IMAGES")
print("="*70)

def load_image_grayscale(filepath, target_size=(128, 128)):
    """Load image as grayscale"""
    img = Image.open(filepath).resize(target_size)
    img_array = np.array(img).astype(np.float32) / 255.0
    if len(img_array.shape) == 2:
        img_array = np.expand_dims(img_array, axis=-1)
    return img_array

print("\nLoading training data...")
X_train_v2 = np.array([load_image_grayscale(fp) for fp in train_df_v2['filepath']])
y_train_v2 = train_df_v2['label'].values

print("Loading validation data...")
X_val_v2 = np.array([load_image_grayscale(fp) for fp in val_df_v2['filepath']])
y_val_v2 = val_df_v2['label'].values

print("Loading test data...")
X_test_v2 = np.array([load_image_grayscale(fp) for fp in test_df_v2['filepath']])
y_test_v2 = test_df_v2['label'].values

print(f"\n‚úì Data loaded:")
print(f"  X_train: {X_train_v2.shape}")
print(f"  X_val:   {X_val_v2.shape}")
print(f"  X_test:  {X_test_v2.shape}")

# STEP 3: Extract Features for Random Forest
print("\n" + "="*70)
print("EXTRACTING FEATURES FOR RANDOM FOREST")
print("="*70)

from skimage.feature import graycomatrix, graycoprops, local_binary_pattern
from scipy import stats
import cv2

def extract_comprehensive_features(image):
    """
    Extract comprehensive texture and intensity features
    that can distinguish contamination
    """
    # Convert to uint8 for feature extraction
    img = (image.squeeze() * 255).astype(np.uint8)

    features = []

    # ===== 1. INTENSITY STATISTICS =====
    features.extend([
        np.mean(img),           # Mean intensity
        np.std(img),            # Standard deviation
        np.min(img),            # Minimum intensity
        np.max(img),            # Maximum intensity
        np.median(img),         # Median intensity
        stats.skew(img.flatten()),  # Skewness
        stats.kurtosis(img.flatten())  # Kurtosis
    ])

    # ===== 2. TEXTURE FEATURES (GLCM) =====
    # Gray Level Co-occurrence Matrix
    glcm = graycomatrix(
        img,
        distances=[1, 2],
        angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],
        levels=256,
        symmetric=True,
        normed=True
    )

    # Calculate texture properties
    for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']:
        prop_values = graycoprops(glcm, prop).flatten()
        features.extend([
            np.mean(prop_values),
            np.std(prop_values)
        ])

    # ===== 3. LOCAL BINARY PATTERN (LBP) =====
    # Captures local texture patterns
    radius = 3
    n_points = 8 * radius
    lbp = local_binary_pattern(img, n_points, radius, method='uniform')

    # LBP histogram
    n_bins = int(lbp.max() + 1)
    lbp_hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)
    features.extend([
        np.mean(lbp_hist),
        np.std(lbp_hist),
        np.max(lbp_hist)
    ])

    # ===== 4. EDGE FEATURES =====
    edges = cv2.Canny(img, 50, 150)
    features.extend([
        np.sum(edges > 0) / edges.size,  # Edge density
        np.std(edges)                     # Edge variation
    ])

    # ===== 5. GRADIENT FEATURES =====
    gradient_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)
    gradient_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)
    gradient_magnitude = np.sqrt(gradient_x**2 + gradient_y**2)

    features.extend([
        np.mean(gradient_magnitude),
        np.std(gradient_magnitude)
    ])

    # ===== 6. FREQUENCY DOMAIN FEATURES (FFT) =====
    fft = np.fft.fft2(img)
    fft_shift = np.fft.fftshift(fft)
    magnitude_spectrum = np.abs(fft_shift)

    features.extend([
        np.mean(magnitude_spectrum),
        np.std(magnitude_spectrum)
    ])

    return np.array(features)

print("\nExtracting features from training data...")
X_train_features = np.array([extract_comprehensive_features(img) for img in X_train_v2])
print(f"‚úì Training features extracted: {X_train_features.shape}")

print("\nExtracting features from validation data...")
X_val_features = np.array([extract_comprehensive_features(img) for img in X_val_v2])
print(f"‚úì Validation features extracted: {X_val_features.shape}")

print("\nExtracting features from test data...")
X_test_features = np.array([extract_comprehensive_features(img) for img in X_test_v2])
print(f"‚úì Test features extracted: {X_test_features.shape}")

print(f"\nTotal features per image: {X_train_features.shape[1]}")

# STEP 4: Train Random Forest Classifier
print("\n" + "="*70)
print("TRAINING RANDOM FOREST CLASSIFIER")
print("="*70)

from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

# Standardize features (important for better performance)
print("\nStandardizing features...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_features)
X_val_scaled = scaler.transform(X_val_features)
X_test_scaled = scaler.transform(X_test_features)

# Train Random Forest
print("\nTraining Random Forest...")
rf_model = RandomForestClassifier(
    n_estimators=200,        # Number of trees
    max_depth=15,            # Maximum depth
    min_samples_split=4,     # Minimum samples to split
    min_samples_leaf=2,      # Minimum samples in leaf
    max_features='sqrt',     # Number of features to consider
    random_state=42,
    class_weight='balanced', # Handle class imbalance
    n_jobs=-1,               # Use all CPU cores
    verbose=1
)

rf_model.fit(X_train_scaled, y_train_v2)

print("\n‚úì Training complete!")
print(f"  Number of trees: {rf_model.n_estimators}")
print(f"  Number of features: {X_train_features.shape[1]}")

# STEP 5: Evaluate the Model
print("\n" + "="*70)
print("MODEL EVALUATION")
print("="*70)

# Validation predictions
print("\n--- VALIDATION SET ---")
val_pred = rf_model.predict(X_val_scaled)
val_pred_proba = rf_model.predict_proba(X_val_scaled)[:, 1]

val_acc = np.mean(val_pred == y_val_v2)
print(f"Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)")

# Test predictions
print("\n--- TEST SET ---")
test_pred = rf_model.predict(X_test_scaled)
test_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]

test_acc = np.mean(test_pred == y_test_v2)
print(f"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")

print(f"\nPrediction Details:")
print(f"  True labels:      {y_test_v2}")
print(f"  Predicted labels: {test_pred}")
print(f"  Probabilities:    {np.round(test_pred_proba, 3)}")

# Check if predicting both classes
unique_preds = np.unique(test_pred)
if len(unique_preds) == 1:
    print(f"\n‚ö†Ô∏è  WARNING: Model only predicts class {unique_preds[0]}")
else:
    print(f"\n‚úÖ Model predicts both classes!")

# Detailed classification report
print("\n" + "="*70)
print("DETAILED CLASSIFICATION REPORT")
print("="*70)
print(classification_report(y_test_v2, test_pred,
                           target_names=['Normal', 'Contaminated'],
                           digits=4))

# Confusion Matrix
cm = confusion_matrix(y_test_v2, test_pred)
print(f"\nConfusion Matrix:")
print(f"                  Predicted")
print(f"                Normal  Contam")
print(f"  Actual Normal    {cm[0,0]:3d}     {cm[0,1]:3d}")
print(f"  Actual Contam    {cm[1,0]:3d}     {cm[1,1]:3d}")

# Calculate additional metrics
from sklearn.metrics import precision_score, recall_score, f1_score

precision = precision_score(y_test_v2, test_pred)
recall = recall_score(y_test_v2, test_pred)
f1 = f1_score(y_test_v2, test_pred)

print(f"\nKey Metrics:")
print(f"  Precision: {precision:.4f}")
print(f"  Recall:    {recall:.4f}")
print(f"  F1-Score:  {f1:.4f}")

# ROC AUC if we have both classes
if len(np.unique(y_test_v2)) > 1:
    try:
        auc = roc_auc_score(y_test_v2, test_pred_proba)
        print(f"  ROC AUC:   {auc:.4f}")
    except:
        print(f"  ROC AUC:   N/A")

# STEP 6: Visualize Results
print("\n" + "="*70)
print("VISUALIZING RESULTS")
print("="*70)

import matplotlib.pyplot as plt

# Feature importance
feature_names = (
    ['Mean', 'Std', 'Min', 'Max', 'Median', 'Skew', 'Kurtosis'] +
    [f'GLCM_{p}_{s}' for p in ['contrast', 'dissim', 'homogen', 'energy', 'corr', 'ASM']
     for s in ['mean', 'std']] +
    ['LBP_mean', 'LBP_std', 'LBP_max'] +
    ['EdgeDensity', 'EdgeVar'] +
    ['GradMean', 'GradStd'] +
    ['FFT_mean', 'FFT_std']
)

importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1][:15]  # Top 15 features

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('Random Forest Contamination Detector - Results', fontsize=16, fontweight='bold')

# Plot 1: Feature Importance
axes[0, 0].barh(range(15), importances[indices][::-1], color='steelblue')
axes[0, 0].set_yticks(range(15))
axes[0, 0].set_yticklabels([feature_names[i] for i in indices][::-1], fontsize=9)
axes[0, 0].set_xlabel('Importance', fontsize=11)
axes[0, 0].set_title('Top 15 Most Important Features', fontsize=12, fontweight='bold')
axes[0, 0].grid(axis='x', alpha=0.3)

# Plot 2: Confusion Matrix
cm_display = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
im = axes[0, 1].imshow(cm_display, cmap='Blues', aspect='auto')
axes[0, 1].set_xticks([0, 1])
axes[0, 1].set_yticks([0, 1])
axes[0, 1].set_xticklabels(['Normal', 'Contaminated'])
axes[0, 1].set_yticklabels(['Normal', 'Contaminated'])
axes[0, 1].set_xlabel('Predicted', fontsize=11)
axes[0, 1].set_ylabel('Actual', fontsize=11)
axes[0, 1].set_title('Confusion Matrix (Normalized)', fontsize=12, fontweight='bold')

# Add text annotations
for i in range(2):
    for j in range(2):
        text = axes[0, 1].text(j, i, f'{cm[i, j]}\n({cm_display[i, j]:.2%})',
                              ha="center", va="center", color="white" if cm_display[i, j] > 0.5 else "black",
                              fontsize=11, fontweight='bold')

# Plot 3: Test Predictions
axes[1, 0].scatter(range(len(y_test_v2)), test_pred_proba,
                  c=['green' if y == 0 else 'red' for y in y_test_v2],
                  s=100, alpha=0.6, edgecolors='black')
axes[1, 0].axhline(y=0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')
axes[1, 0].set_xlabel('Test Sample Index', fontsize=11)
axes[1, 0].set_ylabel('Predicted Probability (Contaminated)', fontsize=11)
axes[1, 0].set_title('Test Set Predictions', fontsize=12, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Plot 4: Sample Images with Predictions
axes[1, 1].axis('off')
inner_grid = axes[1, 1].inset_axes([0, 0, 1, 1])
inner_grid.axis('off')

# Create mini subplots
mini_axes = [plt.subplot(4, 3, i+1) for i in range(min(12, len(X_test_v2)))]

for idx, ax in enumerate(mini_axes):
    if idx < len(X_test_v2):
        ax.imshow(X_test_v2[idx].squeeze(), cmap='gray')
        true_label = "Normal" if y_test_v2[idx] == 0 else "Contam"
        pred_label = "Normal" if test_pred[idx] == 0 else "Contam"
        color = 'green' if y_test_v2[idx] == test_pred[idx] else 'red'
        ax.set_title(f'T:{true_label} P:{pred_label}\n{test_pred_proba[idx]:.2f}',
                    fontsize=7, color=color, fontweight='bold')
        ax.axis('off')

plt.tight_layout()
plt.savefig('random_forest_results.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n‚úì Visualization saved as 'random_forest_results.png'")

# ============================================================================
# COMPLETE BACTERIAL CONTAMINATION DETECTION PIPELINE
# Using Random Forest on Realistic Synthetic Contamination Data
# ============================================================================

import numpy as np
import pandas as pd
from PIL import Image
import glob
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (classification_report, confusion_matrix,
                             precision_score, recall_score, f1_score, roc_auc_score)
from skimage.feature import graycomatrix, graycoprops, local_binary_pattern
from scipy import stats
import cv2

print("="*70)
print("BACTERIAL CONTAMINATION DETECTION - COMPLETE PIPELINE")
print("="*70)

# ============================================================================
# STEP 1: CREATE REALISTIC CONTAMINATION DATA (if not already created)
# ============================================================================

print("\n" + "="*70)
print("STEP 1: CHECKING/CREATING CONTAMINATION DATA")
print("="*70)

from scipy import ndimage

# Check if contamination data already exists
if not os.path.exists('contamination_data_realistic') or len(glob.glob('contamination_data_realistic/*.tif')) < 30:
    print("\nCreating realistic contamination data...")

    os.makedirs('contamination_data_realistic', exist_ok=True)

    s_aureus = [f for f in glob.glob('bacterial_data/training/source/JE2*.tif')
                if '_NR' not in f]
    e_coli = glob.glob('bacterial_data/training/source/pos*.tif')

    print(f"Found {len(s_aureus)} S. aureus images")
    print(f"Found {len(e_coli)} E. coli images")

    def create_realistic_contamination(cocci_path, rods_path, contamination_level='medium'):
        """Create realistic contamination by adding patches of cocci to rods image"""
        cocci = np.array(Image.open(cocci_path))
        rods = np.array(Image.open(rods_path))

        # Resize cocci to match rods
        if cocci.shape != rods.shape:
            cocci = np.array(Image.fromarray(cocci).resize((rods.shape[1], rods.shape[0])))

        # Start with rods as the base
        output = np.copy(rods)
        h, w = cocci.shape

        # Determine contamination parameters
        if contamination_level == 'light':
            num_patches = np.random.randint(1, 3)
            size_range = (40, 80)
        elif contamination_level == 'medium':
            num_patches = np.random.randint(2, 4)
            size_range = (60, 120)
        else:  # heavy
            num_patches = np.random.randint(3, 6)
            size_range = (80, 150)

        # Add patches
        for _ in range(num_patches):
            center_y = np.random.randint(size_range[1], h - size_range[1])
            center_x = np.random.randint(size_range[1], w - size_range[1])
            radius_y = np.random.randint(size_range[0], size_range[1])
            radius_x = np.random.randint(size_range[0], size_range[1])

            # Create elliptical mask
            y, x = np.ogrid[:h, :w]
            mask = ((x - center_x)**2 / radius_x**2 +
                    (y - center_y)**2 / radius_y**2) <= 1

            # Smooth edges
            mask_smooth = ndimage.gaussian_filter(mask.astype(float), sigma=5)

            # Blend
            output = (mask_smooth[:, :, np.newaxis] * cocci[:, :, np.newaxis] +
                      (1 - mask_smooth[:, :, np.newaxis]) * output[:, :, np.newaxis]).squeeze()
            output = output.astype(np.uint8)

        return output

    # Generate contamination images
    num_light = 10
    num_medium = 15
    num_heavy = 10

    print(f"\nGenerating contamination images:")
    print(f"  Light:  {num_light}")
    print(f"  Medium: {num_medium}")
    print(f"  Heavy:  {num_heavy}")

    for i in range(num_light):
        cocci_path = np.random.choice(s_aureus)
        rods_path = np.random.choice(e_coli)
        mixed = create_realistic_contamination(cocci_path, rods_path, 'light')
        Image.fromarray(mixed).save(f'contamination_data_realistic/light_{i:03d}.tif')

    for i in range(num_medium):
        cocci_path = np.random.choice(s_aureus)
        rods_path = np.random.choice(e_coli)
        mixed = create_realistic_contamination(cocci_path, rods_path, 'medium')
        Image.fromarray(mixed).save(f'contamination_data_realistic/medium_{i:03d}.tif')

    for i in range(num_heavy):
        cocci_path = np.random.choice(s_aureus)
        rods_path = np.random.choice(e_coli)
        mixed = create_realistic_contamination(cocci_path, rods_path, 'heavy')
        Image.fromarray(mixed).save(f'contamination_data_realistic/heavy_{i:03d}.tif')

    print("‚úì Contamination data created!")
else:
    print("\n‚úì Contamination data already exists!")

# ============================================================================
# STEP 2: CREATE DATASET
# ============================================================================

print("\n" + "="*70)
print("STEP 2: CREATING DATASET")
print("="*70)

# Collect all file paths and labels
data = []

# Normal samples (label = 0)
normal_files = glob.glob('bacterial_data/training/source/pos*.tif')
for filepath in normal_files:
    data.append({'filepath': filepath, 'label': 0, 'category': 'normal'})

# Contaminated samples (label = 1)
contam_files = (glob.glob('contamination_data_realistic/light_*.tif') +
                glob.glob('contamination_data_realistic/medium_*.tif') +
                glob.glob('contamination_data_realistic/heavy_*.tif'))
for filepath in contam_files:
    data.append({'filepath': filepath, 'label': 1, 'category': 'contaminated'})

# Create DataFrame
df = pd.DataFrame(data)

print(f"\nDataset Summary:")
print(f"  Total images:     {len(df)}")
print(f"  Normal:           {len(df[df['label'] == 0])} images")
print(f"  Contaminated:     {len(df[df['label'] == 1])} images")

# Split data
train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])

print(f"\nData Split:")
print(f"  Training:   {len(train_df)} images")
print(f"    Normal:       {len(train_df[train_df['label']==0])}")
print(f"    Contaminated: {len(train_df[train_df['label']==1])}")
print(f"  Validation: {len(val_df)} images")
print(f"  Test:       {len(test_df)} images")

# ============================================================================
# STEP 3: LOAD IMAGES
# ============================================================================

print("\n" + "="*70)
print("STEP 3: LOADING IMAGES")
print("="*70)

def load_image(filepath, target_size=(128, 128)):
    """Load and preprocess image"""
    img = Image.open(filepath).resize(target_size)
    img_array = np.array(img).astype(np.float32) / 255.0
    if len(img_array.shape) == 2:
        img_array = np.expand_dims(img_array, axis=-1)
    return img_array

print("\nLoading training data...")
X_train = np.array([load_image(fp) for fp in train_df['filepath']])
y_train = train_df['label'].values

print("Loading validation data...")
X_val = np.array([load_image(fp) for fp in val_df['filepath']])
y_val = val_df['label'].values

print("Loading test data...")
X_test = np.array([load_image(fp) for fp in test_df['filepath']])
y_test = test_df['label'].values

print(f"\n‚úì Data loaded:")
print(f"  X_train: {X_train.shape}, y_train: {y_train.shape}")
print(f"  X_val:   {X_val.shape}, y_val:   {y_val.shape}")
print(f"  X_test:  {X_test.shape}, y_test:  {y_test.shape}")

# ============================================================================
# STEP 4: EXTRACT FEATURES FOR RANDOM FOREST
# ============================================================================

print("\n" + "="*70)
print("STEP 4: EXTRACTING FEATURES")
print("="*70)

def extract_features(image):
    """Extract comprehensive features for classification"""
    img = (image.squeeze() * 255).astype(np.uint8)
    features = []

    # 1. Intensity statistics
    features.extend([
        np.mean(img), np.std(img), np.min(img), np.max(img), np.median(img),
        stats.skew(img.flatten()), stats.kurtosis(img.flatten())
    ])

    # 2. Texture features (GLCM)
    glcm = graycomatrix(img, distances=[1, 2], angles=[0, np.pi/4, np.pi/2, 3*np.pi/4],
                        levels=256, symmetric=True, normed=True)

    for prop in ['contrast', 'dissimilarity', 'homogeneity', 'energy', 'correlation', 'ASM']:
        prop_values = graycoprops(glcm, prop).flatten()
        features.extend([np.mean(prop_values), np.std(prop_values)])

    # 3. Local Binary Pattern
    radius = 3
    n_points = 8 * radius
    lbp = local_binary_pattern(img, n_points, radius, method='uniform')
    n_bins = int(lbp.max() + 1)
    lbp_hist, _ = np.histogram(lbp.ravel(), bins=n_bins, range=(0, n_bins), density=True)
    features.extend([np.mean(lbp_hist), np.std(lbp_hist), np.max(lbp_hist)])

    # 4. Edge features
    edges = cv2.Canny(img, 50, 150)
    features.extend([np.sum(edges > 0) / edges.size, np.std(edges)])

    # 5. Gradient features
    gradient_x = cv2.Sobel(img, cv2.CV_64F, 1, 0, ksize=3)
    gradient_y = cv2.Sobel(img, cv2.CV_64F, 0, 1, ksize=3)
    gradient_magnitude = np.sqrt(gradient_x**2 + gradient_y**2)
    features.extend([np.mean(gradient_magnitude), np.std(gradient_magnitude)])

    # 6. FFT features
    fft = np.fft.fft2(img)
    fft_shift = np.fft.fftshift(fft)
    magnitude_spectrum = np.abs(fft_shift)
    features.extend([np.mean(magnitude_spectrum), np.std(magnitude_spectrum)])

    return np.array(features)

print("\nExtracting features...")
X_train_features = np.array([extract_features(img) for img in X_train])
X_val_features = np.array([extract_features(img) for img in X_val])
X_test_features = np.array([extract_features(img) for img in X_test])

print(f"‚úì Features extracted: {X_train_features.shape[1]} features per image")

# ============================================================================
# STEP 5: STANDARDIZE FEATURES
# ============================================================================

print("\n" + "="*70)
print("STEP 5: STANDARDIZING FEATURES")
print("="*70)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_features)
X_val_scaled = scaler.transform(X_val_features)
X_test_scaled = scaler.transform(X_test_features)

print("‚úì Features standardized")

# ============================================================================
# STEP 6: TRAIN RANDOM FOREST
# ============================================================================

print("\n" + "="*70)
print("STEP 6: TRAINING RANDOM FOREST CLASSIFIER")
print("="*70)

rf_model = RandomForestClassifier(
    n_estimators=200,
    max_depth=15,
    min_samples_split=4,
    min_samples_leaf=2,
    max_features='sqrt',
    random_state=42,
    class_weight='balanced',
    n_jobs=-1
)

print("\nTraining...")
rf_model.fit(X_train_scaled, y_train)
print("‚úì Training complete!")

# ============================================================================
# STEP 7: EVALUATE MODEL
# ============================================================================

print("\n" + "="*70)
print("STEP 7: MODEL EVALUATION")
print("="*70)

# Validation predictions
print("\n--- VALIDATION SET ---")
val_pred = rf_model.predict(X_val_scaled)
val_pred_proba = rf_model.predict_proba(X_val_scaled)[:, 1]
val_acc = np.mean(val_pred == y_val)
print(f"Validation Accuracy: {val_acc:.4f} ({val_acc*100:.2f}%)")

# Test predictions
print("\n--- TEST SET ---")
test_pred = rf_model.predict(X_test_scaled)
test_pred_proba = rf_model.predict_proba(X_test_scaled)[:, 1]
test_acc = np.mean(test_pred == y_test)
print(f"Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)")

print(f"\nPrediction Details:")
print(f"  True labels:      {y_test}")
print(f"  Predicted labels: {test_pred}")
print(f"  Probabilities:    {np.round(test_pred_proba, 3)}")

# Check if predicting both classes
unique_preds = np.unique(test_pred)
if len(unique_preds) == 1:
    print(f"\n‚ö†Ô∏è  WARNING: Model only predicts class {unique_preds[0]}")
else:
    print(f"\n‚úÖ Model predicts both classes!")

# Detailed metrics
print("\n" + "="*70)
print("DETAILED CLASSIFICATION REPORT")
print("="*70)
print(classification_report(y_test, test_pred,
                           target_names=['Normal', 'Contaminated'],
                           digits=4))

# Confusion Matrix
cm = confusion_matrix(y_test, test_pred)
print(f"\nConfusion Matrix:")
print(f"                  Predicted")
print(f"                Normal  Contam")
print(f"  Actual Normal    {cm[0,0]:3d}     {cm[0,1]:3d}")
print(f"  Actual Contam    {cm[1,0]:3d}     {cm[1,1]:3d}")

# Additional metrics
if len(unique_preds) > 1:
    precision = precision_score(y_test, test_pred)
    recall = recall_score(y_test, test_pred)
    f1 = f1_score(y_test, test_pred)

    print(f"\nKey Metrics:")
    print(f"  Precision: {precision:.4f}")
    print(f"  Recall:    {recall:.4f}")
    print(f"  F1-Score:  {f1:.4f}")

    if len(np.unique(y_test)) > 1:
        try:
            auc = roc_auc_score(y_test, test_pred_proba)
            print(f"  ROC AUC:   {auc:.4f}")
        except:
            pass

# ============================================================================
# STEP 8: VISUALIZE RESULTS
# ============================================================================

print("\n" + "="*70)
print("STEP 8: VISUALIZING RESULTS")
print("="*70)

fig, axes = plt.subplots(2, 2, figsize=(16, 12))
fig.suptitle('Random Forest Contamination Detector - Results', fontsize=16, fontweight='bold')

# Plot 1: Feature Importance (Top 15)
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1][:15]

axes[0, 0].barh(range(15), importances[indices][::-1], color='steelblue')
axes[0, 0].set_yticks(range(15))
axes[0, 0].set_yticklabels([f'Feature {i}' for i in indices][::-1], fontsize=9)
axes[0, 0].set_xlabel('Importance', fontsize=11)
axes[0, 0].set_title('Top 15 Most Important Features', fontsize=12, fontweight='bold')
axes[0, 0].grid(axis='x', alpha=0.3)

# Plot 2: Confusion Matrix
cm_display = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
im = axes[0, 1].imshow(cm_display, cmap='Blues', aspect='auto')
axes[0, 1].set_xticks([0, 1])
axes[0, 1].set_yticks([0, 1])
axes[0, 1].set_xticklabels(['Normal', 'Contaminated'])
axes[0, 1].set_yticklabels(['Normal', 'Contaminated'])
axes[0, 1].set_xlabel('Predicted', fontsize=11)
axes[0, 1].set_ylabel('Actual', fontsize=11)
axes[0, 1].set_title('Confusion Matrix (Normalized)', fontsize=12, fontweight='bold')

for i in range(2):
    for j in range(2):
        text = axes[0, 1].text(j, i, f'{cm[i, j]}\n({cm_display[i, j]:.2%})',
                              ha="center", va="center",
                              color="white" if cm_display[i, j] > 0.5 else "black",
                              fontsize=11, fontweight='bold')

# Plot 3: Prediction probabilities
axes[1, 0].scatter(range(len(y_test)), test_pred_proba,
                  c=['green' if y == 0 else 'red' for y in y_test],
                  s=100, alpha=0.6, edgecolors='black')
axes[1, 0].axhline(y=0.5, color='black', linestyle='--', linewidth=2, label='Threshold')
axes[1, 0].set_xlabel('Test Sample Index', fontsize=11)
axes[1, 0].set_ylabel('Predicted Probability (Contaminated)', fontsize=11)
axes[1, 0].set_title('Test Set Predictions', fontsize=12, fontweight='bold')
axes[1, 0].legend()
axes[1, 0].grid(True, alpha=0.3)

# Plot 4: Sample predictions
axes[1, 1].axis('off')
n_samples = min(9, len(X_test))
for idx in range(n_samples):
    row = idx // 3
    col = idx % 3
    ax = plt.subplot(2, 2, 4, frameon=False)
    inner_ax = plt.subplot(3, 3, idx + 1)

    inner_ax.imshow(X_test[idx].squeeze(), cmap='gray')
    true_label = "Normal" if y_test[idx] == 0 else "Contam"
    pred_label = "Normal" if test_pred[idx] == 0 else "Contam"
    color = 'green' if y_test[idx] == test_pred[idx] else 'red'
    inner_ax.set_title(f'T:{true_label} P:{pred_label}\n{test_pred_proba[idx]:.2f}',
                      fontsize=8, color=color, fontweight='bold')
    inner_ax.axis('off')

plt.tight_layout()
plt.savefig('random_forest_contamination_results.png', dpi=150, bbox_inches='tight')
plt.show()

print("\n‚úì Results visualization saved!")

# ============================================================================
# SUMMARY
# ============================================================================

print("\n" + "="*70)
print("PIPELINE COMPLETE - SUMMARY")
print("="*70)
print(f"\nüìä Dataset:")
print(f"  Total images:     {len(df)}")
print(f"  Training:         {len(train_df)}")
print(f"  Test:             {len(test_df)}")

print(f"\nüéØ Model Performance:")
print(f"  Test Accuracy:    {test_acc:.4f} ({test_acc*100:.2f}%)")
if len(unique_preds) > 1:
    print(f"  F1-Score:         {f1:.4f}")
    print(f"  Status:           ‚úÖ Predicts both classes")
else:
    print(f"  Status:           ‚ö†Ô∏è  Only predicts class {unique_preds[0]}")

print(f"\nüìÅ Output:")
print(f"  Visualization:    random_forest_contamination_results.png")
print("="*70)

# FINAL PRESENTATION SLIDE
fig = plt.figure(figsize=(16, 10))
gs = fig.add_gridspec(3, 3, hspace=0.4, wspace=0.3)

fig.suptitle('Automated Bacterial Contamination Detection System',
             fontsize=18, fontweight='bold', y=0.98)

# 1. Sample Images (Top row)
ax1 = fig.add_subplot(gs[0, 0])
ax1.imshow(X_test[y_test==0][0].squeeze() if len(X_test[y_test==0]) > 0 else X_train[y_train==0][0].squeeze(), cmap='gray')
ax1.set_title('‚úÖ Normal Culture\n(Pure E. coli)', fontsize=11, fontweight='bold', color='green')
ax1.axis('off')

ax2 = fig.add_subplot(gs[0, 1])
ax2.imshow(X_test[y_test==1][0].squeeze() if len(X_test[y_test==1]) > 0 else X_train[y_train==1][0].squeeze(), cmap='gray')
ax2.set_title('‚ö†Ô∏è Contaminated Culture\n(Mixed Species)', fontsize=11, fontweight='bold', color='red')
ax2.axis('off')

# Metrics box
ax3 = fig.add_subplot(gs[0, 2])
ax3.axis('off')
metrics_text = f"""
MODEL PERFORMANCE

Accuracy:  {test_acc*100:.1f}%
F1-Score:  {f1:.3f}
Precision: {precision:.3f}
Recall:    {recall:.3f}

DATASET
Training:  {len(train_df)} images
Test:      {len(test_df)} images
Features:  {X_train_features.shape[1]}
"""
ax3.text(0.1, 0.5, metrics_text, fontsize=11, family='monospace',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
         verticalalignment='center')

# 2. Confusion Matrix (Middle left)
ax4 = fig.add_subplot(gs[1, :2])
cm_display = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
im = ax4.imshow(cm_display, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)
ax4.set_xticks([0, 1])
ax4.set_yticks([0, 1])
ax4.set_xticklabels(['Normal', 'Contaminated'], fontsize=12)
ax4.set_yticklabels(['Normal', 'Contaminated'], fontsize=12)
ax4.set_xlabel('Predicted Label', fontsize=12, fontweight='bold')
ax4.set_ylabel('True Label', fontsize=12, fontweight='bold')
ax4.set_title('Confusion Matrix', fontsize=13, fontweight='bold')

for i in range(2):
    for j in range(2):
        text = ax4.text(j, i, f'{cm[i, j]}\n({cm_display[i, j]:.1%})',
                       ha="center", va="center",
                       color="white" if cm_display[i, j] < 0.5 else "black",
                       fontsize=14, fontweight='bold')

# 3. Feature Importance (Middle right)
ax5 = fig.add_subplot(gs[1, 2])
top_10_idx = np.argsort(importances)[-10:]
ax5.barh(range(10), importances[top_10_idx], color='steelblue')
ax5.set_yticks(range(10))
ax5.set_yticklabels([f'Feature {i}' for i in top_10_idx], fontsize=8)
ax5.set_xlabel('Importance', fontsize=10)
ax5.set_title('Top 10 Features', fontsize=11, fontweight='bold')
ax5.grid(axis='x', alpha=0.3)

# 4. Workflow Diagram (Bottom)
ax6 = fig.add_subplot(gs[2, :])
ax6.axis('off')
workflow_text = """
PIPELINE WORKFLOW
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. IMAGE INPUT ‚Üí 2. FEATURE EXTRACTION ‚Üí 3. CLASSIFICATION ‚Üí 4. RESULT

   Microscopy       ‚Ä¢ Intensity stats      Random Forest      ‚úÖ Normal
   Images           ‚Ä¢ Texture (GLCM)       200 trees          ‚ö†Ô∏è  Contaminated
   (128x128)        ‚Ä¢ Edge detection       Balanced
                    ‚Ä¢ Gradients
                    ‚Ä¢ FFT
"""
ax6.text(0.5, 0.5, workflow_text, fontsize=10, family='monospace',
         ha='center', va='center',
         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.3))

plt.savefig('hackathon_final_presentation.png', dpi=200, bbox_inches='tight')
plt.show()

print("\n‚úì Final presentation slide saved as: hackathon_final_presentation.png")